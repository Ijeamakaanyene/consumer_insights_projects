---
title: "economic burden analysis"
output: html_document
---
```{r}
library(dplyr)
library(ggplot2)
library(readr)

```

# Census Block Data
Source: https://www.safegraph.com/open-census-data
Dataset of demographic data on the census block level which I downloaded to my computer. This is used to create an economic burden index based on the following:
1. Long commute times
2. Unemployment
3. Home price to income ratio
4. Percent uninsured population

# Metric: Long commute times

```{r}
cbg_b08 = read_csv("/Users/ijeamakaanyene/curology/environ_data/Census Block Data/safegraph_open_census_data/data/cbg_b08.csv")

```

```{r}
# Census block group data related to commute times

travel_time = data.frame(cbg_b08$census_block_group) 
colnames(travel_time) = "census_block_group"
travel_time$total = cbg_b08$B08303e1
travel_time$min_40_44 = cbg_b08$B08303e10
travel_time$min_45_59 = cbg_b08$B08303e11
travel_time$min_60_89 = cbg_b08$B08303e12
travel_time$min_M90 = cbg_b08$B08303e13
travel_time$min_L5 = cbg_b08$B08303e2
travel_time$min_5_9 = cbg_b08$B08303e3
travel_time$min_10_14 = cbg_b08$B08303e4
travel_time$min_15_19 = cbg_b08$B08303e5
travel_time$min_20_24 = cbg_b08$B08303e6
travel_time$min_25_29 = cbg_b08$B08303e7
travel_time$min_30_34 = cbg_b08$B08303e8
travel_time$min_35_39 = cbg_b08$B08303e9

```

```{r}
# Metric created - % of population w >= 45 minutes commute
travel_time = travel_time %>%
  mutate(M45_travel_time = (min_45_59 + min_60_89 +  min_M90),
         perc_M45 = M45_travel_time / total)
```

# Metric: Unemployment

```{r}
cbg_b23 = read_csv("/Users/ijeamakaanyene/curology/environ_data/Census Block Data/safegraph_open_census_data/data/cbg_b23.csv")

```

```{r}
# Census block group data related to unemployment
no_work = data.frame(cbg_b23$census_block_group)
colnames(no_work) = "census_block_group"

no_work$total = cbg_b23$B23027e1
no_work$y20_24 = cbg_b23$B23027e11
no_work$y25_44 = cbg_b23$B23027e16
no_work$y45_54 = cbg_b23$B23027e21
no_work$y55_64 = cbg_b23$B23027e26
no_work$y65_69 = cbg_b23$B23027e31
no_work$y70 = cbg_b23$B23027e36
no_work$y16_19 = cbg_b23$B23027e6
```


```{r}
# Metric created - % of population who did not work in the past 12 months

no_work = mutate(no_work,
                 perc_no_work = (y20_24 + y25_44 + y45_54 + y55_64 + y65_69 + y70 + y16_19) / total )

```


# Metric: Health Insurance Coverage
```{r}
cbg_b27 = read_csv("/Users/ijeamakaanyene/curology/environ_data/Census Block Data/safegraph_open_census_data/data/cbg_b27.csv")
```

```{r}
# Census block group data related to health insurance
health_insur = data.frame(cbg_b27$census_block_group)
colnames(health_insur) = "census_block_group"

health_insur$total = cbg_b27$B27010e1
health_insur$Y18 = cbg_b27$B27010e17
health_insur$Y18_34 = cbg_b27$B27010e33
health_insur$Y35_64 = cbg_b27$B27010e50
health_insur$Y65 = cbg_b27$B27010e66
```

```{r}
# Metric created - % of population that is uninsured
health_insur = mutate(health_insur, 
                      perc_no_healthinsur = (Y18 + Y18_34 + Y35_64 + Y65) / total)
```

#Metric: >30% of income going to rent 

```{r}
cbg_b25 = read_csv("/Users/ijeamakaanyene/curology/environ_data/Census Block Data/safegraph_open_census_data/data/cbg_b25.csv")

```

```{r}
# Census block group data related to income rent ratio
income_rent = data.frame(cbg_b25$census_block_group)
colnames(income_rent) = "census_block_group"

income_rent$total = cbg_b25$B25070e1
income_rent$G50 = cbg_b25$B25070e10
income_rent$I30_34 = cbg_b25$B25070e7
income_rent$I35_39 = cbg_b25$B25070e8
income_rent$I40_49 = cbg_b25$B25070e9

```

```{r}
# Metric created - % of population that has >30% of income going to rent
income_rent = mutate(income_rent,
                     rent_g30 = (G50 + I30_34 + I35_39 + I40_49) / total)

```


# Combining metrics into one dataframe

```{r}
health_insur_total = select(health_insur, 
                            census_block_group, perc_no_healthinsur)
income_rent_total = select(income_rent, 
                           census_block_group, rent_g30)
no_work_total = select(no_work, 
                       census_block_group, perc_no_work)
travel_time_total = select(travel_time, 
                           census_block_group, perc_M45)

```

```{r}
eburden_ctract = full_join(health_insur_total, income_rent_total, by = c("census_block_group" = "census_block_group")) %>%
  full_join(., no_work_total, by = c("census_block_group" = "census_block_group")) %>%
  full_join(., travel_time_total, by = c("census_block_group" = "census_block_group"))

```

```{r}
eburden_ctract[is.na(eburden_ctract) == TRUE] = 0
```

# Creation of a economic burden index - principal components analysis
```{r}
scaled_df = eburden_ctract %>%
  select(perc_no_healthinsur, rent_g30, perc_no_work, perc_M45)

apply(scaled_df, 2, var, na.rm = TRUE) # Computing the variance of each variable

scaled_df = apply(scaled_df, 2, scale)
```

```{r}
eburden.cov = cov(scaled_df) #Covariance matrix
eburden.eigen = eigen(eburden.cov) #Eigenvalues
str(eburden.eigen)
```

```{r}
phi = eburden.eigen$vectors[,1:2]
phi = -phi #Flipping to a positive direction for interpretation purposes. 
```

```{r}
row.names(phi) = c("perc_no_healthinsur", "rent_g30", "perc_no_work", "perc_M45")
colnames(phi) = c("PC1", "PC2")
phi
# For principle component 1, we see there is loading on all four variables which is expected and all of the loadings are in the same directions.
```

```{r}
round(eburden.eigen$values / sum(eburden.eigen$values) , 2)
# 30% of variance is explained by the first principal compenent which is impacted with socioeconomic indices

```

```{r}
# Calculating principal components score to obtain the values of the components for each observation 
PC1 = as.matrix(scaled_df) %*% phi[,1]
ecburden_index = data.frame(census_block_group = eburden_ctract$census_block_group, PC1)

# Note highest score are rent g30, no health insurance, and percent no work, so index is reflecting areas where that is the worst. 
```

```{r}
# Notes: Census block group needs 12 digits & census tract needs 11 digits. Census block group code is just a subset of the census tract

# Confirming if leading zeros have been dropped
ecburden_index$length_cblock = sapply(ecburden_index[,1], nchar)
filter(ecburden_index, length_cblock < 12) # none have been dropped 

ecburden_index$census_tract = substr(ecburden_index[,1], start = 1, stop = 11)

```

# Customers with acne / clogged pores by zipcode
```{r}
user_zipcodes = read.csv("/Users/ijeamakaanyene/curology/environ_data/Datasets/skincare_zipcodes.csv", stringsAsFactors = FALSE, na.strings = c("", NA))

```

```{r}
# Randomly creating number of customers by zipcode 
number_customers = round(runif(25936, min = 1, max = 2000), digits = 0)
```

```{r}
user_zipcodes$number_customers = number_customers
user_zipcodes = user_zipcodes[,-1]

```

# Crosswalk for zipcode to census tract
```{r}
# This is a crosswalk I obtained from dataworld 
zip_tract = read_csv("/Users/ijeamakaanyene/curology/environ_data/Datasets/ZIP_TRACT_122015.csv")

```

```{r}
# There are zipcodes where the leading zeros were dropped. Creating quick for loop to add them back! 

zip_tract$df_zip_long = NA

for(i in 1:nrow(zip_tract)){
  if(nchar(zip_tract[i,1])==3){
    zip_tract[i,7] = paste("00",zip_tract[i,1], sep = "")
  } else if(nchar(zip_tract[i,1])==4) {
    zip_tract[i,7] = paste("0",zip_tract[i,1], sep = "")
  } else {
    zip_tract[i,7] = zip_tract[i,1]
  }
}
```

```{r}
# There are also tracts where the leading zeroes were dropped. Creating quick for loop to add these back as well. 
zip_tract$tract_long = NA

for(i in 1:nrow(zip_tract)){
  if(nchar(zip_tract[i,2]) == 9){
    zip_tract[i,8] = paste("00", zip_tract[i,2], sep ="")
  } else if(nchar(zip_tract[i,2]) == 10) {
     zip_tract[i,8] = paste("0", zip_tract[i,2], sep ="")
  } else {
    zip_tract[i,8] = zip_tract[i,2]
  }
}


```

```{r}
# Data frame with nunber of pts, zipcode, and census tract
acne_ctract = left_join(user_zipcodes, zip_tract, by = c("zipcode_abbrev" = "df_zip_long")) %>%
  select(zipcode_abbrev, number_customers, tract_long)

acne_ctract = filter(acne_ctract, is.na(tract_long) == FALSE)

```

# Adding economic burden index to users by ctract
```{r}
acne_ctract$tract_long = as.character(acne_ctract$tract_long)
ecburden_index$census_tract = as.character(ecburden_index$census_tract)

str(acne_ctract)
str(ecburden_index)
```

```{r}
acne_ecburden = left_join(acne_ctract, ecburden_index, by = c("tract_long" = "census_tract"))
```

```{r}
# How many actually matched?
sum(is.na(acne_ecburden$PC1) == TRUE) / nrow(acne_ecburden) * 100

#0.027% did not match

```

```{r}
# Zipcodes are showing up multiple times - this is because multiple census tracts match to one zipcode. Thus, recalculating economic burden index as the mean of all economic burden PC scores

acne_ecburden = acne_ecburden %>%
  group_by(zipcode_abbrev, number_customers) %>%
  summarise(mean_ecburden_index = mean(PC1, na.rm = TRUE))
```


# Statistical analysis

```{r}
glm_ecburden = glm(acne_ecburden$number_customers ~ acne_ecburden$mean_ecburden_index, family = "poisson", data = acne_ecburden)

summary(glm_ecburden)

exp(glm_ecburden$coefficients)

# In this fake dataset of patients, a one unit increase in Economic Burden Index score, is associated with an increase of 0.99 in the number of acne patients at the zip code level 
```

With Poission Regression models you need to worry about overdispersion where the variance is much larger than the mean. In this model, there isn't overdispersion because of the randomly generated data. But let's pretend there is and switch to a negative binomial model. 

# negative binomial model 

```{r}
glm_ecburden_nb = MASS::glm.nb(acne_ecburden$number_customers ~ acne_ecburden$mean_ecburden_index, data = acne_ecburden)

summary(glm_ecburden_nb)

exp(glm_ecburden_nb$coefficients)
exp(confint(glm_ecburden_nb))


# In this fake dataset, a one unit increase in Economic Burden Index score, is associated with an increase of 0.99 in the number of acne patients at the zip code level (however, the confidence interval includes the null thus it is not statistical significant)
```

Another option is to calculate the Poisson Regression with Robust Standard Errors. But this was not done in this analysis. 

