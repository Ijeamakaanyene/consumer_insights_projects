---
title: "economic burden analysis"
output: html_document
---
```{r}
library(dplyr)
library(ggplot2)
library(readr)

```

# Census Block Data
Source: https://www.safegraph.com/open-census-data
Dataset of demographic data on the census block level which I downloaded to my computer. This is used to create an economic burden index based on the following:
1. Long commute times
2. Unemployment
3. Home price to income ratio
4. Percent uninsured population

# Metric: Long commute times

```{r}
cbg_b08 = read_csv("/Users/ijeamakaanyene/curology/environ_data/Census Block Data/safegraph_open_census_data/data/cbg_b08.csv")

```

```{r}
# Census block group data related to commute times

travel_time = data.frame(cbg_b08$census_block_group) 
colnames(travel_time) = "census_block_group"
travel_time$total = cbg_b08$B08303e1
travel_time$min_40_44 = cbg_b08$B08303e10
travel_time$min_45_59 = cbg_b08$B08303e11
travel_time$min_60_89 = cbg_b08$B08303e12
travel_time$min_M90 = cbg_b08$B08303e13
travel_time$min_L5 = cbg_b08$B08303e2
travel_time$min_5_9 = cbg_b08$B08303e3
travel_time$min_10_14 = cbg_b08$B08303e4
travel_time$min_15_19 = cbg_b08$B08303e5
travel_time$min_20_24 = cbg_b08$B08303e6
travel_time$min_25_29 = cbg_b08$B08303e7
travel_time$min_30_34 = cbg_b08$B08303e8
travel_time$min_35_39 = cbg_b08$B08303e9

```

```{r}
# Metric created - % of population w >= 45 minutes commute
travel_time = travel_time %>%
  mutate(M45_travel_time = (min_45_59 + min_60_89 +  min_M90),
         perc_M45 = M45_travel_time / total)
```

# Metric: Unemployment

```{r}
cbg_b23 = read_csv("/Users/ijeamakaanyene/curology/environ_data/Census Block Data/safegraph_open_census_data/data/cbg_b23.csv")

```

```{r}
# Census block group data related to unemployment
no_work = data.frame(cbg_b23$census_block_group)
colnames(no_work) = "census_block_group"

no_work$total = cbg_b23$B23027e1
no_work$y20_24 = cbg_b23$B23027e11
no_work$y25_44 = cbg_b23$B23027e16
no_work$y45_54 = cbg_b23$B23027e21
no_work$y55_64 = cbg_b23$B23027e26
no_work$y65_69 = cbg_b23$B23027e31
no_work$y70 = cbg_b23$B23027e36
no_work$y16_19 = cbg_b23$B23027e6
```


```{r}
# Metric created - % of population who did not work in the past 12 months
no_work = mutate(no_work,
                 perc_no_work = (y20_24 + y25_44 + y45_54 + y55_64 + y65_69 + y70 + y16_19) / total )

```


# Metric: Health Insurance Coverage
```{r}
cbg_b27 = read_csv("/Users/ijeamakaanyene/curology/environ_data/Census Block Data/safegraph_open_census_data/data/cbg_b27.csv")
```

```{r}
# Census block group data related to health insurance
health_insur = data.frame(cbg_b27$census_block_group)
colnames(health_insur) = "census_block_group"

health_insur$total = cbg_b27$B27010e1
health_insur$Y18 = cbg_b27$B27010e17
health_insur$Y18_34 = cbg_b27$B27010e33
health_insur$Y35_64 = cbg_b27$B27010e50
health_insur$Y65 = cbg_b27$B27010e66
```

```{r}
# Metric created - % of population that is uninsured
health_insur = mutate(health_insur, 
                      perc_no_healthinsur = (Y18 + Y18_34 + Y35_64 + Y65) / total)
```

#Metric: >30% of income going to rent 

```{r}
cbg_b25 = read_csv("/Users/ijeamakaanyene/curology/environ_data/Census Block Data/safegraph_open_census_data/data/cbg_b25.csv")

```

```{r}
# Census block group data related to income rent ratio
income_rent = data.frame(cbg_b25$census_block_group)
colnames(income_rent) = "census_block_group"

income_rent$total = cbg_b25$B25070e1
income_rent$G50 = cbg_b25$B25070e10
income_rent$I30_34 = cbg_b25$B25070e7
income_rent$I35_39 = cbg_b25$B25070e8
income_rent$I40_49 = cbg_b25$B25070e9

```

```{r}
# Metric created - % of population that has >30% of income going to rent
income_rent = mutate(income_rent,
                     rent_g30 = (G50 + I30_34 + I35_39 + I40_49) / total)

```


# Combining metrics into one dataframe

```{r}
health_insur_total = select(health_insur, 
                            census_block_group, perc_no_healthinsur)
income_rent_total = select(income_rent, 
                           census_block_group, rent_g30)
no_work_total = select(no_work, 
                       census_block_group, perc_no_work)
travel_time_total = select(travel_time, 
                           census_block_group, perc_M45)

```

```{r}
eburden_ctract = full_join(health_insur_total, income_rent_total, by = c("census_block_group" = "census_block_group")) %>%
  full_join(., no_work_total, by = c("census_block_group" = "census_block_group")) %>%
  full_join(., travel_time_total, by = c("census_block_group" = "census_block_group"))

```

```{r}
eburden_ctract[is.na(eburden_ctract) == TRUE] = 0
```

# Creation of a economic burden index - principal components analysis
```{r}
scaled_df = eburden_ctract %>%
  select(perc_no_healthinsur, rent_g30, perc_no_work, perc_M45)

apply(scaled_df, 2, var, na.rm = TRUE) # Computing the variance of each variable

scaled_df = apply(scaled_df, 2, scale)
```

```{r}
eburden.cov = cov(scaled_df) #Covariance matrix
eburden.eigen = eigen(eburden.cov) #Eigenvalues
str(eburden.eigen)
```

```{r}
phi = eburden.eigen$vectors[,1:2]
phi = -phi #Flipping to a positive direction for interpretation purposes. 
```

```{r}
row.names(phi) = c("perc_no_healthinsur", "rent_g30", "perc_no_work", "perc_M45")
colnames(phi) = c("PC1", "PC2")
phi
# For principle component 1, we see there is loading on all four variables which is expected and all of the loadings are in the same directions.
```

```{r}
round(eburden.eigen$values / sum(eburden.eigen$values) , 2)
# 32% of variance is explained by the first principal compenent which is on par with socioeconomic indices
```

```{r}
# Calculating principal components score to obtain the values of the components for each observation 
PC1 = as.matrix(scaled_df) %*% phi[,1]
ecburden_index = data.frame(census_block_group = eburden_ctract$census_block_group, PC1, stringsAsFactors = FALSE)

# Note highest score are rent g30, no health insurance, and percent no work, so index is reflecting areas where that is the worst. 
```

```{r}
# Changing structure of census block group
eburden_ctract$census_block_group = as.character(eburden_ctract$census_block_group)
str(eburden_ctract$census_block_group)
```

```{r}
# Notes: Census block group needs 12 digits & census tract needs 11 digits. Census block group code is just a subset of the census tract

# Confirming if leading zeros have been dropped
ecburden_index$length_cblock = apply(ecburden_index[1], 1, nchar)
filter(ecburden_index, length_cblock < 12) # none have been dropped 

ecburden_index$census_tract = substr(ecburden_index[,1], start = 1, stop = 11)

```

# Customers with acne / clogged pores by zipcode
```{r}
user_zipcodes = read_csv("/Users/ijeamakaanyene/curology/environ_data/Datasets/skincare_zipcodes.csv")

```

```{r}
# Randomly creating number of customers by zipcode 
set.seed(11717)
number_customers = round(runif(nrow(user_zipcodes), min = 1, max = 2000), digits = 0)
```

```{r}
user_zipcodes$number_customers = number_customers
user_zipcodes = user_zipcodes[,-1]

```

# Crosswalk for zipcode to census tract
```{r}
# This is a crosswalk of zipcodes to census tract obtained from dataworld 
zip_tract = read_csv("/Users/ijeamakaanyene/curology/environ_data/Datasets/ZIP_TRACT_122015.csv")

```

```{r}
# Zipcodes were leaded as characters, but confirming they are at the correct length

zip_tract$length_zip = apply(zip_tract[1], 1, nchar)
nrow(filter(zip_tract, length_zip < 5)) # Returned 0 so no zeros dropped
```

```{r}
# Checking whether there are census tracts where the leading zeroes were dropped. 
zip_tract$length_tract = apply(zip_tract[2], 1, nchar)

nrow(filter(zip_tract, length_tract < 11)) 
# 26,704 rows that are missing zeros, so need to add them back!
```

```{r}
# There are also tracts where the leading zeroes were dropped. Creating quick for loop to add these back as well. 
zip_tract$tract_long = NA

for(i in 1:nrow(zip_tract)){
  if(nchar(zip_tract[i,2]) == 9){
    zip_tract[i,9] = paste("00", zip_tract[i,2], sep ="")
    
  } else if(nchar(zip_tract[i,2]) == 10) {
     zip_tract[i,9] = paste("0", zip_tract[i,2], sep ="")
     
  } else {
    zip_tract[i,9] = zip_tract[i,2]
  }
}


```

```{r}
# Data frame with nunber of pts, zipcode, and census tract
acne_ctract = left_join(user_zipcodes, zip_tract, by = c("zipcode_abbrev" = "ZIP")) %>%
  select(zipcode_abbrev, number_customers, tract_long)

acne_ctract = filter(acne_ctract, is.na(tract_long) == FALSE)

```

# Adding economic burden index to users by ctract
```{r}
acne_ctract$tract_long = as.character(acne_ctract$tract_long)
ecburden_index$census_tract = as.character(ecburden_index$census_tract)

str(acne_ctract$tract_long)
str(ecburden_index$census_tract)
```

```{r}
acne_ecburden = left_join(acne_ctract, ecburden_index, by = c("tract_long" = "census_tract"))
```

```{r}
# How many actually matched?
sum(is.na(acne_ecburden$PC1) == TRUE) / nrow(acne_ecburden) * 100

#0.027% did not match

```

```{r}
# Zipcodes are showing up multiple times - this is because multiple census tracts match to one zipcode. Thus, recalculating economic burden index as the mean of all economic burden PC scores

acne_ecburden = acne_ecburden %>%
  group_by(zipcode_abbrev, number_customers) %>%
  summarise(mean_ecburden_index = mean(PC1, na.rm = TRUE))
```

# Review Variables
Even though the number of customers is randomly generated numbers, it is still good habit to plot your variables to better understand and identify any potential issues or interesting nuances!

```{r}
ggplot(acne_ecburden, aes(y = number_customers, x = mean_ecburden_index)) + geom_point()

# Well this confirms the random number generator worked correctly. There is one zipcode with quite a high economic burden index (near 4) and 11 zipcodes with quite low economic burden index (less than -3). Most zipcodes fall between -2 and 2 for the index. 
```

# Regression Models

```{r}
glm_ecburden = glm(number_customers ~ mean_ecburden_index, family = "poisson", data = acne_ecburden)

summary(glm_ecburden)
```

In Poisson Regression models, when using them to calculate risk ratios you often have to be concerned with overdispersion where the variance is much larger than the mean. SO lets check for overdispersion by calculating the coefficients and confidence interval with and without robust standard errors. 

```{r}
# Poisson Regression 
coef_pois = coef(glm_ecburden)
ci_pois = confint.default(glm_ecburden) 
exp(cbind(coef_pois, ci_pois))[2,]
```

```{r}
# Poisson Regression with Robust Standard Errors
coef_robust = doBy::esticon(glm_ecburden, diag(length(coef(glm_ecburden))))
exp_robust = exp(cbind(coef_robust$Estimate, coef_robust$Lower, coef_robust$Upper))
rownames(exp_robust) = names(coef(glm_ecburden))
colnames(exp_robust) = c("RR", "95% LL", "95% UL")

exp_robust[2,]

```


Comparing the estimates and confidence intervals calculated with and without robust errors, the confidence intervals are approximately the same pointing towards the fact that there is no overdispersion. Thus for a one unit change in the economic burden index, the risk of acne changes by 0.999. Essentially the risk decreases, with an increase in economic burden index. However, 0.999 is close to null that I would feel comfortable asying there is minimal to no association between number of acne patients and economic burden index. 

Some concerns/ acknowledgements with this analysis:
- Each subject is not followed for the same amount of time. Typically you would account for varying lengths of observation time in the model. Thus the estimates is biased. But - its fake data. 



